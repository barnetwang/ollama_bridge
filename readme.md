# æ™ºæ…§å‹ AI ä»£ç†ä¼ºæœå™¨ (An Intelligent AI Proxy Server)

é€™æ˜¯ä¸€å€‹åŸºæ–¼ Python å’Œ Flask çš„æ™ºæ…§ä»£ç†ä¼ºæœå™¨ï¼Œå°ˆç‚ºå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾Œç«¯ï¼ˆå¦‚ Ollamaï¼‰è¨­è¨ˆã€‚å®ƒä¸åƒ…åƒ…æ˜¯ä¸€å€‹è«‹æ±‚è½‰ç™¼å™¨ï¼Œè€Œæ˜¯ä¸€å€‹**æ™ºæ…§ã€å¤šæ¨¡æ…‹ã€å¤šè§’è‰²çš„ AI ä»£ç† (AI Agent)**ã€‚

å®ƒçš„æ ¸å¿ƒèƒ½åŠ›æ˜¯**ç†è§£ä½¿ç”¨è€…æ„åœ–**ï¼Œä¸¦**å‹•æ…‹åœ°çµ„ç¹”ä¸€å€‹è™›æ“¬å°ˆå®¶åœ˜éšŠ**ä¾†ç”Ÿæˆé«˜å“è³ªçš„ã€ç¬¦åˆæƒ…å¢ƒçš„å›ç­”ã€‚å®ƒé‚„èƒ½ç„¡ç¸«è™•ç†åœ–æ–‡æ··åˆè«‹æ±‚ï¼Œä¸¦é€šé**é€šç”¨é©é…å™¨æ¶æ§‹**è¼•é¬†æ“´å±•ä»¥æ”¯æ´å¤šç¨®å‰ç«¯æ‡‰ç”¨ã€‚

---

## âœ¨ ä¸»è¦åŠŸèƒ½ (Key Features)

- **ğŸ§  å‹•æ…‹å°ˆå®¶è§’è‰²èåˆ (Dynamic Expert Persona Fusion)**:
  èƒ½æ ¹æ“šç”¨æˆ¶çš„è·¨é ˜åŸŸå•é¡Œï¼ˆä¾‹å¦‚ã€Œå¯«ä¸€ç¯‡é—œæ–¼é”æ–‡è¥¿æ‰‹è¡“çš„å°èªªã€ï¼‰ï¼Œè‡ªå‹•é¸æ“‡ä¸¦èåˆå¤šä½å°ˆå®¶ï¼ˆå¦‚ `Doctor` + `Writer`ï¼‰çš„èƒ½åŠ›ï¼Œç”Ÿæˆå…¼å…·æ·±åº¦èˆ‡å‰µæ„çš„å›ç­”ã€‚

- **ğŸ”§ @å‰ç¶´å·¥å…·èª¿ç”¨ (Prefix-based Tool Calling)**:
  é€šéç°¡å–®çš„ `@ç¶²è·¯æœå°‹` æŒ‡ä»¤ï¼Œå³å¯è§¸ç™¼ä»£ç†ä¼ºæœå™¨åŸ·è¡Œ Google æœå°‹ï¼Œå°‡å³æ™‚çš„ç¶²è·¯è³‡è¨Šä½œç‚ºä¸Šä¸‹æ–‡æä¾›çµ¦ LLMï¼Œä½¿å…¶èƒ½å¤ å›ç­”é—œæ–¼æœ€æ–°äº‹ä»¶çš„å•é¡Œã€‚

- **ğŸ–¼ï¸ ç„¡ç¸«å¤šæ¨¡æ…‹è™•ç† (Seamless Multimodal Handling)**:
  è‡ªå‹•è­˜åˆ¥åœ–ç‰‡ä¸¦æ¡ç”¨ã€Œçœ‹åœ–èªªè©± -> å°ˆå®¶æ€è€ƒç¸½çµã€çš„æµç¨‹ï¼Œèƒ½å¤ å°‡å°ˆå®¶è§’è‰²èƒ½åŠ›æ‡‰ç”¨æ–¼åœ–æ–‡æ··åˆçš„å•ç­”ä¸­ã€‚

- **ğŸ”Œ é€šç”¨é©é…å™¨æ¶æ§‹ (Universal Adapter Architecture)**:
  å¯é€éåœ¨ `adapters.py` ä¸­æ–°å¢é©é…å™¨ï¼Œå¿«é€Ÿæ”¯æ´æ–°çš„ API æ ¼å¼ï¼Œç›®å‰å…§å»ºæ”¯æ´ **LobeChat** å’Œ **OpenAI ç›¸å®¹å®¢æˆ¶ç«¯** (å¦‚ Cherry Studio)ã€‚

- **âœï¸ æ˜“æ–¼ç¶­è­·çš„ Prompt åº« (Externalized Prompt Library)**:
  æ‰€æœ‰å°ˆå®¶è§’è‰²çš„èƒ½åŠ›å’Œè¡Œç‚ºæº–å‰‡éƒ½å®šç¾©åœ¨ `prompts/` è³‡æ–™å¤¾ä¸‹çš„ `.txt` æª”æ¡ˆä¸­ï¼Œæ–°å¢æˆ–ä¿®æ”¹ AI çš„è§’è‰²å®Œå…¨ç„¡éœ€æ›´å‹•ç¨‹å¼ç¢¼ã€‚

- **ğŸš€ æµå¼å›æ‡‰ (Streaming Responses)**:
  å°‡æœ€çµ‚ç­”æ¡ˆä»¥ä¸²æµæ–¹å¼å‚³å›å®¢æˆ¶ç«¯ï¼Œæä¾›æµæš¢çš„ä½¿ç”¨è€…é«”é©—ã€‚

## ğŸ›ï¸ æ¶æ§‹è¨­è¨ˆ (Architecture)

æœ¬å°ˆæ¡ˆçš„æ ¸å¿ƒæ˜¯**å°‡ã€Œæ ¸å¿ƒæ¥­å‹™é‚è¼¯ã€èˆ‡ã€ŒAPI é©é…é‚è¼¯ã€å¾¹åº•åˆ†é›¢**ã€‚

1.  **æ ¸å¿ƒé‚è¼¯ (`proxy_server.py`)**: è² è²¬æ‰€æœ‰é«˜éšæ™ºæ…§ä»»å‹™ï¼š
    *   **æ„åœ–è­˜åˆ¥**: åˆ¤æ–·ä½¿ç”¨è€…æ˜¯å¦æƒ³èª¿ç”¨å·¥å…·ï¼ˆå¦‚ `@ç¶²è·¯æœå°‹`ï¼‰ã€‚
    *   **å°ˆå®¶æ±ºç­–**: èª¿ç”¨ LLM åˆ†æå•é¡Œï¼Œæ±ºå®šéœ€è¦å“ªä¸€ä½æˆ–å“ªå¹¾ä½å°ˆå®¶ä¾†å”ä½œå›ç­”ã€‚
    *   **Prompt èåˆ**: å‹•æ…‹åœ°å°‡å°ˆå®¶ Prompt å’Œå·¥å…·æœå°‹çµæœèåˆæˆä¸€å€‹å¼·å¤§çš„ã€Œå…ƒ Promptã€ã€‚
    *   **å¤šæ¨¡æ…‹å”èª¿**: è™•ç†åœ–æ–‡è«‹æ±‚çš„ç‰¹æ®Šæµç¨‹ã€‚

2.  **é©é…å™¨ (`adapters.py`)**: é‡å°æ¯ä¸€å€‹å®¢æˆ¶ç«¯ï¼Œéƒ½æœ‰ä¸€å€‹å°ˆå±¬çš„é©é…å™¨ï¼Œè² è²¬ï¼š
    *   **è«‹æ±‚ç¿»è­¯**: å°‡å®¢æˆ¶ç«¯çš„ç¨ç‰¹ JSON æ ¼å¼ç¿»è­¯æˆå…§éƒ¨æ¨™æº–æ ¼å¼ã€‚
    *   **å›æ‡‰ç«¯é»é¸æ“‡**: å‘Šè¨´æ ¸å¿ƒé‚è¼¯æ‡‰ä½¿ç”¨å“ªå€‹ API ç«¯é» (`/api/chat` æˆ– `/v1/chat/completions`) ä¾†å›æ‡‰ï¼Œç¢ºä¿æ ¼å¼å…¼å®¹ã€‚

3.  **Prompt åº« (`prompts/`)**:
    *   ä»¥ç¨ç«‹ `.txt` æª”æ¡ˆå½¢å¼å­˜æ”¾æ‰€æœ‰å°ˆå®¶è§’è‰²çš„å®šç¾©ã€‚

## ğŸ› ï¸ å®‰è£èˆ‡è¨­å®š (Installation & Setup)

#### 1. å‰ç½®éœ€æ±‚ (Prerequisites)

- **Ollama**: è«‹ç¢ºä¿æ‚¨çš„ç³»çµ±å·²å®‰è£ [Ollama](https://ollama.com/)ã€‚
- **Ollama æ¨¡å‹**: ä¸‹è¼‰æœ¬å°ˆæ¡ˆæ‰€éœ€çš„æ¨¡å‹ã€‚
  ```bash
  # æ€è€ƒæ¨¡å‹ (Thinking Model)
  ollama pull gpt-oss:20b
  # è¦–è¦ºæ¨¡å‹ (Vision Model)
  ollama pull gemma3:4b
  ```
  *æ‚¨å¯ä»¥åœ¨ `proxy_server.py` çš„é ‚éƒ¨ä¿®æ”¹ `THINKING_MODEL` å’Œ `VISION_MODEL` è®Šæ•¸ã€‚*

#### 2. Google æœå°‹ API (å¯é¸ï¼Œä½†æ¨è–¦)

- è‹¥è¦å•Ÿç”¨ `@ç¶²è·¯æœå°‹` åŠŸèƒ½ï¼Œæ‚¨éœ€è¦ä¸€å€‹ Google Custom Search API é‡‘é‘°ã€‚
  1.  åœ¨ [Google Cloud Console](https://console.cloud.google.com/) å•Ÿç”¨ "Custom Search API"ã€‚
  2.  å»ºç«‹ä¸€å€‹ **API é‡‘é‘°**ã€‚
  3.  åœ¨ [å¯ç¨‹å¼åŒ–æœå°‹å¼•æ“](https://programmablesearchengine.google.com/controlpanel/all) å»ºç«‹ä¸€å€‹æœå°‹å¼•æ“ï¼ˆé¸æ“‡ã€Œåœ¨æ•´å€‹ç¶²è·¯ä¸Šæœå°‹ã€ï¼‰ï¼Œä¸¦ç²å–å…¶ **æœå°‹å¼•æ“ ID**ã€‚
  4.  åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹ï¼Œå»ºç«‹ä¸€å€‹ `.env` æª”æ¡ˆï¼Œä¸¦å¡«å…¥æ‚¨çš„é‡‘é‘°ï¼š
      ```.env
      GOOGLE_API_KEY="æ‚¨çš„_API_é‡‘é‘°"
      GOOGLE_CSE_ID="æ‚¨çš„_æœå°‹å¼•æ“_ID"
      ```

#### 3. å®‰è£ Python ä¾è³´

- åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹ï¼Œå»ºç«‹ `requirements.txt` æª”æ¡ˆï¼š
  ```txt
  Flask
  requests
  python-dotenv
  google-api-python-client
  ```
- åŸ·è¡Œå®‰è£ï¼š
  ```bash
  pip install -r requirements.txt
  ```

#### 4. å»ºç«‹ Prompt åº«

- åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹ï¼Œå»ºç«‹ä¸€å€‹åç‚º `prompts` çš„è³‡æ–™å¤¾ã€‚
- åœ¨æ­¤è³‡æ–™å¤¾å…§ï¼Œå»ºç«‹æ‚¨çš„å°ˆå®¶è§’è‰²æª”æ¡ˆï¼ˆæª”åå¿…é ˆç‚º**è‹±æ–‡**ï¼‰ã€‚ä¾‹å¦‚ï¼š`Doctor.txt`, `Writer.txt`, `Assistant.txt`ã€‚

## ğŸš€ å¦‚ä½•ä½¿ç”¨ (How to Use)

1.  **å•Ÿå‹•ä»£ç†ä¼ºæœå™¨**:
    ```bash
    python proxy_server.py
    ```
    ä¼ºæœå™¨å°‡åœ¨ `http://localhost:5000` ä¸Šç›£è½ã€‚

2.  **è¨­å®šæ‚¨çš„å®¢æˆ¶ç«¯**:
    - **å°æ–¼ LobeChat (æ¨è–¦)**:
      - API ç«¯é»: `http://localhost:5000`
      - æ¨¡å‹åç¨±: `gpt-oss:20b` (æˆ–å…¶ä»–æ‚¨è¨­å®šçš„æ€è€ƒæ¨¡å‹)
    - **å°æ–¼ OpenAI ç›¸å®¹å®¢æˆ¶ç«¯**:
      - API Base URL: `http://localhost:5000/v1`
      - æ¨¡å‹åç¨±: `gpt-oss:20b`

3.  **é–‹å§‹å°è©±**:
    - **å¸¸è¦å•é¡Œ**: ã€Œé”æ–‡è¥¿æ‰‹è¡“æ˜¯ä»€éº¼ï¼Ÿã€ -> å°‡ç”± `Doctor` è§’è‰²å›ç­”ã€‚
    - **è·¨é ˜åŸŸå•é¡Œ**: ã€Œå¯«ä¸€ç¯‡é—œæ–¼é”æ–‡è¥¿æ‰‹è¡“çš„å°èªªã€ -> å°‡ç”± `Doctor` å’Œ `Writer` åœ˜éšŠå”åŒå›ç­”ã€‚
    - **ç¶²è·¯æœå°‹**: ã€Œ@ç¶²è·¯æœå°‹ ç‰¹æ–¯æ‹‰ Cybertruck çš„ç‰¹é»ã€ -> å°‡è§¸ç™¼ Google æœå°‹ä¸¦åŸºæ–¼å³æ™‚è³‡è¨Šå›ç­”ã€‚

## ğŸ§© å¦‚ä½•æ“´å±• (How to Extend)

### æ–°å¢ä¸€ä½å°ˆå®¶

1.  åœ¨ `prompts` è³‡æ–™å¤¾ä¸­ï¼Œæ–°å¢ä¸€å€‹ `.txt` æª”æ¡ˆï¼Œä¾‹å¦‚ `Historian.txt`ã€‚
2.  åœ¨æª”æ¡ˆå…§ï¼Œç·¨å¯«é€™ä½å°ˆå®¶çš„è§’è‰²å®šç¾©å’Œè¡Œç‚ºæº–å‰‡ã€‚
3.  é‡å•Ÿä»£ç†ä¼ºæœå™¨å³å¯ï¼

### æ”¯æ´ä¸€å€‹æ–°çš„å®¢æˆ¶ç«¯

1.  **åµéŒ¯** "Future Assist" çš„ API è·¯å¾‘å’Œ JSON çµæ§‹ã€‚
2.  åœ¨ `adapters.py` ä¸­ï¼Œç‚ºå…¶**å»ºç«‹**ä¸€å€‹æ–°çš„ `FutureAssistAdapter` é¡ã€‚
3.  åœ¨ `adapters.py` çš„ `ADAPTER_REGISTRY` åˆ—è¡¨ä¸­ï¼Œ**è¨»å†Š**é€™å€‹æ–°é©é…å™¨ã€‚
4.  é‡å•Ÿä¼ºæœå™¨å³å¯ç”Ÿæ•ˆï¼

---
## âš–ï¸ æˆæ¬Šèˆ‡æ„Ÿè¬ (License & Acknowledgements)

æ­¤å°ˆæ¡ˆç‚ºé–‹æºé …ç›®ï¼Œä½¿ç”¨äº†å¤šå€‹ç¬¬ä¸‰æ–¹å‡½å¼åº«ã€‚è«‹ä»”ç´°é–±è®€å…¶æˆæ¬Šæ¢æ¬¾ï¼Œåœ¨å•†æ¥­ç”¨é€”å‰å‹™å¿…å¯©è¦–æ¸…æ¥šã€‚

# Adapter AI Proxy (English Version)

# æ™ºæ…§å‹ AI ä»£ç†ä¼ºæœå™¨ (An Intelligent AI Proxy Server)

é€™æ˜¯ä¸€å€‹åŸºæ–¼ Python å’Œ Flask çš„æ™ºæ…§ä»£ç†ä¼ºæœå™¨ï¼Œå°ˆç‚ºå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾Œç«¯ï¼ˆå¦‚ Ollamaï¼‰è¨­è¨ˆã€‚å®ƒä¸åƒ…åƒ…æ˜¯ä¸€å€‹è«‹æ±‚è½‰ç™¼å™¨ï¼Œè€Œæ˜¯ä¸€å€‹**æ™ºæ…§ã€å¤šæ¨¡æ…‹ã€å¤šè§’è‰²çš„ AI ä»£ç† (AI Agent)**ã€‚

å®ƒçš„æ ¸å¿ƒèƒ½åŠ›æ˜¯**ç†è§£ä½¿ç”¨è€…æ„åœ–**ï¼Œä¸¦**å‹•æ…‹åœ°çµ„ç¹”ä¸€å€‹è™›æ“¬å°ˆå®¶åœ˜éšŠ**ä¾†ç”Ÿæˆé«˜å“è³ªçš„ã€ç¬¦åˆæƒ…å¢ƒçš„å›ç­”ã€‚å®ƒé‚„èƒ½ç„¡ç¸«è™•ç†åœ–æ–‡æ··åˆè«‹æ±‚ï¼Œä¸¦é€šé**é€šç”¨é©é…å™¨æ¶æ§‹**è¼•é¬†æ“´å±•ä»¥æ”¯æ´å¤šç¨®å‰ç«¯æ‡‰ç”¨ã€‚

---
---

<br>

# [ English Version ]

This is an intelligent proxy server built with Python and Flask, designed for Large Language Model (LLM) backends like Ollama. It functions as an **intelligent, multimodal, multi-persona AI Agent** that goes beyond simple request forwarding.

Its core capability is to **understand user intent** and **dynamically assemble a virtual team of experts** to generate high-quality, context-aware responses. It seamlessly handles mixed text-and-image requests and is easily extensible to support various front-end applications through its **Universal Adapter Architecture**.

---

## âœ¨ Key Features

- **ğŸ§  Dynamic Expert Persona Fusion**:
  Based on a user's cross-disciplinary query (e.g., "write a short story about Da Vinci surgery"), it can automatically select and fuse the capabilities of multiple experts (like `Doctor` + `Writer`) to generate a response with both depth and creativity.

- **ğŸ”§ Prefix-based Tool Calling**:
  Using a simple `@web_search` command, the proxy can trigger a Google Search to provide the LLM with real-time information, enabling it to answer questions about recent events.

- **ğŸ–¼ï¸ Seamless Multimodal Handling**:
  Automatically detects images and uses a "describe-then-think" workflow, applying the selected expert persona's skills to answer complex vision-language questions.

- **ğŸ”Œ Universal Adapter Architecture**:
  Quickly support new API formats by adding adapters in `adapters.py`. It comes with built-in support for **LobeChat** and **OpenAI-compatible clients** (like Cherry Studio).

- **âœï¸ Externalized & Easy-to-Maintain Prompt Library**:
  All expert personas, including their skills and behavioral guidelines, are defined in simple `.txt` files within the `prompts/` directory. Adding or modifying the AI's roles requires no code changes.

- **ğŸš€ Streaming Responses**:
  Streams the final answer back to the client for a smooth, real-time user experience.

## ğŸ›ï¸ Architecture

The core principle of this project is the **complete separation of "Core Logic" from "API Adaptation Logic."**

1.  **Core Logic (`proxy_server.py`)**: Handles all high-level intelligent tasks:
    *   **Intent Recognition**: Detects if the user wants to invoke a tool (e.g., `@web_search`).
    *   **Expert Decision**: Calls the LLM to analyze the query and decide which expert(s) are needed for the task.
    *   **Prompt Fusion**: Dynamically combines expert prompts and tool results into a powerful "meta-prompt."
    *   **Multimodal Coordination**: Manages the specialized workflow for text-and-image requests.

2.  **Adapters (`adapters.py`)**: A dedicated adapter exists for each client, responsible for:
    *   **Request Translation**: Translating the client's unique JSON format ("dialect") into the proxy's internal standard format.
    *   **Response Endpoint Selection**: Informing the core logic which API endpoint (`/api/chat` or `/v1/chat/completions`) to use for the response, ensuring format compatibility.

3.  **Prompt Library (`prompts/`)**:
    *   Contains all expert persona definitions as individual `.txt` files.

## ğŸ› ï¸ Installation & Setup

#### 1. Prerequisites

- **Ollama**: Ensure you have [Ollama](https://ollama.com/) installed on your system.
- **Ollama Models**: Pull the required models.
  ```bash
  # Thinking Model
  ollama pull gpt-oss:20b
  # Vision Model
  ollama pull gemma3:4b
  ```
  *You can change the `THINKING_MODEL` and `VISION_MODEL` variables at the top of `proxy_server.py`.*

#### 2. Google Search API (Optional, but recommended)

- To enable the `@web_search` feature, you need a Google Custom Search API key.
  1.  Enable "Custom Search API" in the [Google Cloud Console](https://console.cloud.google.com/).
  2.  Create an **API Key**.
  3.  Create a search engine in the [Programmable Search Engine](https://programmablesearchengine.google.com/controlpanel/all) console (select "Search the entire web") and get its **Search engine ID**.
  4.  Create a `.env` file in the project root and add your credentials:
      ```.env
      GOOGLE_API_KEY="YOUR_API_KEY"
      GOOGLE_CSE_ID="YOUR_SEARCH_ENGINE_ID"
      ```

#### 3. Install Python Dependencies

- Create a `requirements.txt` file in the project root:
  ```txt
  Flask
  requests
  python-dotenv
  google-api-python-client
  ```
- Run the installation:
  ```bash
  pip install -r requirements.txt
  ```

#### 4. Create the Prompt Library

- Create a folder named `prompts` in the project root.
- Inside this folder, create your expert persona files with **English filenames**. For example:
    - `prompts/Doctor.txt`
    - `prompts/Writer.txt`
    - `prompts/Software_Engineer.txt`
    - `prompts/Assistant.txt` (This is the default fallback persona and is required).

## ğŸš€ How to Use

1.  **Start the Proxy Server**:
    ```bash
    python proxy_server.py
    ```
    The server will be listening on `http://localhost:5000`.

2.  **Configure Your Client**:
    - **For LobeChat (Recommended)**:
      - API Endpoint: `http://localhost:5000`
      - Path: `api/chat`
      - Model Name: `gpt-oss:20b` (or your configured thinking model)
    - **For OpenAI-Compatible Clients**:
      - API Base URL: `http://localhost:5000/v1`
      - Model Name: `gpt-oss:20b`

3.  **Start Chatting**:
    - **Normal Query**: "What is Da Vinci surgery?" -> Will be answered by the `Doctor` persona.
    - **Cross-Disciplinary Query**: "Write a short story about Da Vinci surgery" -> Will be answered by a `Doctor` and `Writer` team.
    - **Web Search**: "@web_search What are the features of the Tesla Cybertruck?" -> Will trigger a Google Search and answer based on real-time information.

## ğŸ§© How to Extend

### Adding a New Expert

This is the easiest way to extend the agent's capabilities:
1.  Add a new `.txt` file to the `prompts` folder, e.g., `Historian.txt`.
2.  Write the persona definition and behavioral guidelines inside the file.
3.  Restart the proxy server. The decision-making model will now automatically consider "Historian" as a potential team member!

### Supporting a New Client

1.  **Investigate** the new client's API path (e.g., `/api/v3/chat`) and JSON structure.
2.  **Create** a new `FutureAssistAdapter` class in `adapters.py`.
3.  **Register** this new adapter in the `ADAPTER_REGISTRY` list in `adapters.py`.
4.  Restart the server. It's that simple!

---
## âš–ï¸ License & Acknowledgements

This is an open-source project that utilizes various third-party libraries. Please review their respective licenses carefully before using this code for commercial purposes.
