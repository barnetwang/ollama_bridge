# AI ä»£ç†ä¼ºæœå™¨

é€™æ˜¯ä¸€å€‹åŸºæ–¼ Python å’Œ Flask çš„æ™ºæ…§ä»£ç†ä¼ºæœå™¨ï¼Œå°ˆç‚ºå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾Œç«¯ï¼ˆå¦‚ Ollamaï¼‰è¨­è¨ˆã€‚å…¶æ ¸å¿ƒç‰¹è‰²æ˜¯æ¡ç”¨äº†**é€šç”¨é©é…å™¨æ¶æ§‹ (Universal Adapter Architecture)**ï¼Œä½¿å…¶èƒ½å¤ è¼•é¬†æ“´å……ä»¥æ”¯æ´ä¾†è‡ªä¸åŒå®¢æˆ¶ç«¯ï¼ˆå¦‚ LobeChatã€Cherry Studio ç­‰ï¼‰çš„ API æ ¼å¼ã€‚

æ­¤ä»£ç†ä¼ºæœå™¨èƒ½å¤ æ™ºæ…§åœ°åˆ¤æ–·ä½¿ç”¨è€…è«‹æ±‚æ˜¯å¦åŒ…å«åœ–ç‰‡ï¼Œä¸¦è‡ªå‹•å”èª¿ä¸€å€‹ã€Œæ€è€ƒæ¨¡å‹ã€å’Œä¸€å€‹ã€Œè¦–è¦ºæ¨¡å‹ã€ä¾†è™•ç†å¤šæ¨¡æ…‹è«‹æ±‚ï¼Œæœ€çµ‚å°‡æ ¼å¼åŒ–çš„å›æ‡‰ä¸²æµå›åŸå§‹çš„å®¢æˆ¶ç«¯ã€‚

---

## æ¶æ§‹è¨­è¨ˆ

æœ¬å°ˆæ¡ˆçš„æ ¸å¿ƒæ˜¯**å°‡ã€Œæ ¸å¿ƒæ¥­å‹™é‚è¼¯ã€èˆ‡ã€ŒAPI é©é…é‚è¼¯ã€åˆ†é›¢**ã€‚

1.  **æ ¸å¿ƒé‚è¼¯ (Core Logic)**: é€™æ˜¯æ­¤å°ˆæ¡ˆæœ€æœ‰åƒ¹å€¼çš„éƒ¨åˆ†ï¼Œè² è²¬è™•ç†æ‰€æœ‰èˆ‡æ¨¡å‹äº’å‹•çš„é€šç”¨ä»»å‹™ï¼Œä¾‹å¦‚ï¼š
    *   åˆ¤æ–·è«‹æ±‚æ˜¯å¦éœ€è¦åœ–åƒåˆ†æã€‚
    *   èª¿ç”¨è¦–è¦ºæ¨¡å‹ (`VISION_MODEL`) ä¾†æè¿°åœ–ç‰‡ã€‚
    *   çµ„åˆåœ–ç‰‡æè¿°èˆ‡ä½¿ç”¨è€…å•é¡Œï¼Œèª¿ç”¨æ€è€ƒæ¨¡å‹ (`THINKING_MODEL`) é€²è¡Œæœ€çµ‚å›ç­”ã€‚
    *   è™•ç†æµå¼å›æ‡‰ã€‚

2.  **é©é…å™¨ (Adapters)**: é‡å°æ¯ä¸€å€‹éœ€è¦æ”¯æ´çš„å®¢æˆ¶ç«¯æ‡‰ç”¨ç¨‹å¼ (AP)ï¼Œéƒ½æœ‰ä¸€å€‹å°ˆå±¬çš„é©é…å™¨ã€‚æ¯å€‹é©é…å™¨åªè² è²¬å…©ä»¶äº‹ï¼š
    *   **è«‹æ±‚ç¿»è­¯**: å°‡ç‰¹å®šå®¢æˆ¶ç«¯çš„ API è«‹æ±‚æ ¼å¼ï¼ˆâ€œæ–¹è¨€â€ï¼‰ç¿»è­¯æˆä»£ç†ä¼ºæœå™¨å…§éƒ¨çš„æ¨™æº–æ ¼å¼ã€‚
    *   **å›æ‡‰ç¿»è­¯**: å°‡ä»£ç†ä¼ºæœå™¨çš„æ¨™æº–æµå¼å›æ‡‰ï¼Œè½‰æ›ç‚ºåŸå§‹å®¢æˆ¶ç«¯æœŸæœ›çš„æ ¼å¼å’Œ API ç«¯é»ã€‚

é€™ç¨®è¨­è¨ˆä½¿å¾—æ ¸å¿ƒé‚è¼¯ä¿æŒç©©å®šã€ä¹¾æ·¨ï¼ŒåŒæ™‚è®“æ–°å¢å°å…¶ä»–å®¢æˆ¶ç«¯çš„æ”¯æ´è®Šå¾—éå¸¸ç°¡å–®å’Œä½é¢¨éšªã€‚

## ä¸»è¦åŠŸèƒ½

- **æ™ºæ…§å¤šæ¨¡æ…‹è™•ç†**: è‡ªå‹•è­˜åˆ¥åœ–ç‰‡ä¸¦æ¡ç”¨ã€Œçœ‹åœ–èªªè©± -> æ€è€ƒç¸½çµã€çš„æµç¨‹è™•ç†åœ–æ–‡è«‹æ±‚ã€‚
- **é€šç”¨é©é…å™¨æ¶æ§‹**: å¯é€éåœ¨ `adapters.py` ä¸­æ–°å¢é©é…å™¨ï¼Œå¿«é€Ÿæ”¯æ´æ–°çš„ API æ ¼å¼ã€‚
- **å®¢æˆ¶ç«¯æ”¯æ´**: ç›®å‰å…§å»ºæ”¯æ´ï¼š
  - **LobeChat** (`/api/chat`)
  - **OpenAI ç›¸å®¹å®¢æˆ¶ç«¯** (å¦‚ Cherry Studio) (`/v1/chat/completions`)
- **æ¨¡å‹è‡ªè¨‚**: å¯åœ¨ `proxy_server.py` ä¸­è¼•é¬†æ›´æ›ç”¨æ–¼æ€è€ƒå’Œè¦–è¦ºçš„ Ollama æ¨¡å‹ã€‚
- **æµå¼å›æ‡‰**: å°‡æœ€çµ‚ç­”æ¡ˆä»¥ä¸²æµæ–¹å¼å‚³å›å®¢æˆ¶ç«¯ï¼Œæå‡ä½¿ç”¨è€…é«”é©—ã€‚

## å®‰è£èˆ‡è¨­å®š

1.  **å‰ç½®éœ€æ±‚**:
    *   è«‹ç¢ºä¿æ‚¨çš„ç³»çµ±å·²å®‰è£ [Ollama](https://ollama.com/)ã€‚
    *   ä¸‹è¼‰æœ¬å°ˆæ¡ˆæ‰€éœ€çš„æ¨¡å‹ã€‚é è¨­æ¨¡å‹ç‚ºï¼š
      ```bash
      ollama pull gpt-oss:20b # æ€è€ƒæ¨¡å‹
      ollama pull gemma3:4b    # è¦–è¦ºæ¨¡å‹
      ```
      *æ‚¨å¯ä»¥åœ¨ `proxy_server.py` ä¸­ä¿®æ”¹ `THINKING_MODEL` å’Œ `VISION_MODEL` è®Šæ•¸ã€‚*

2.  **å®‰è£ä¾è³´**:
    åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹ï¼Œé€é `requirements.txt` å®‰è£æ‰€éœ€çš„ Python å¥—ä»¶ã€‚
    ```bash
    pip install -r requirements.txt
    ```

## å¦‚ä½•ä½¿ç”¨

1.  **å•Ÿå‹•ä»£ç†ä¼ºæœå™¨**:
    ```bash
    python proxy_server.py
    ```
    æˆåŠŸå•Ÿå‹•å¾Œï¼Œæ‚¨æœƒçœ‹åˆ°ä¼ºæœå™¨æ­£åœ¨ `http://localhost:5000` ä¸Šç›£è½ã€‚

2.  **è¨­å®šæ‚¨çš„å®¢æˆ¶ç«¯**:
    å°‡æ‚¨çš„ AI å®¢æˆ¶ç«¯ï¼ˆå¦‚ LobeChatï¼‰çš„ Ollama API ä½å€æŒ‡å‘æ­¤ä»£ç†ä¼ºæœå™¨ã€‚

    - **å°æ–¼ LobeChat**:
      - API ä½å€: `http://localhost:5000/api/chat`
      - æ¨¡å‹åç¨±: `gpt-oss:20b` (æˆ–å…¶ä»–æ‚¨åœ¨ä»£ç†ä¸­è¨­å®šçš„æ€è€ƒæ¨¡å‹)

    - **å°æ–¼ OpenAI ç›¸å®¹å®¢æˆ¶ç«¯**:
      - API ä½å€: `http://localhost:5000/v1`
      - æ¨¡å‹åç¨±: `gpt-oss:20b`

## å¦‚ä½•æ”¯æ´ä¸€å€‹æ–°çš„å®¢æˆ¶ç«¯

å‡è¨­æ‚¨æƒ³æ”¯æ´ä¸€å€‹åç‚º "Future Assist" çš„æ–°å®¢æˆ¶ç«¯ï¼Œåªéœ€éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š

1.  **åµéŒ¯å…¶ API æ ¼å¼**: äº†è§£ "Future Assist" å‘¼å«çš„ API è·¯å¾‘ï¼ˆä¾‹å¦‚ `/api/v3/chat`ï¼‰ä»¥åŠå®ƒå‚³é€çš„ JSON çµæ§‹ã€‚

2.  **å»ºç«‹æ–°é©é…å™¨**: åœ¨ `adapters.py` æª”æ¡ˆä¸­ï¼Œå»ºç«‹ä¸€å€‹ç¹¼æ‰¿è‡ª `BaseAdapter` çš„æ–°é¡åˆ¥ã€‚
    ```python
    class FutureAssistAdapter(BaseAdapter):
        name = "future_assist"

        def parse(self):
            # åœ¨æ­¤å¯¦ç¾è§£æ "Future Assist" JSON æ ¼å¼çš„é‚è¼¯
            # ...
            return user_prompt, image_base64
    ```

3.  **è¨»å†Šæ–°é©é…å™¨**: åœ¨ `adapters.py` åº•éƒ¨çš„ `ADAPTERS` å­—å…¸ä¸­ï¼Œå°‡æ–°å®¢æˆ¶ç«¯çš„ API è·¯å¾‘é—œéµå­—èˆ‡æ‚¨å‰›å»ºç«‹çš„é©é…å™¨é¡åˆ¥é—œè¯èµ·ä¾†ã€‚
    ```python
    ADAPTERS = {
        "api/chat": LobeChatAdapter,
        "v1/chat/completions": CherryStudioAdapter,
        "api/v3/chat": FutureAssistAdapter  # æ–°å¢æ­¤è¡Œ
    }
    ```

4.  **é‡å•Ÿä¼ºæœå™¨**: é‡æ–°å•Ÿå‹• `proxy_server.py` å³å¯ç”Ÿæ•ˆã€‚å°±æ˜¯é€™éº¼ç°¡å–®ï¼

---
## âš–ï¸ æˆæ¬Šèˆ‡æ„Ÿè¬

æ­¤å°ˆæ¡ˆç‚ºé–‹æºé …ç›®ï¼Œä½¿ç”¨äº†å¤šå€‹ç¬¬ä¸‰æ–¹å‡½å¼åº«ã€‚
è«‹ä»”ç´°é–±è®€å…¶æˆæ¬Šæ¢æ¬¾ï¼Œåœ¨å•†æ¥­ç”¨é€”å‰å‹™å¿…å¯©è¦–æ¸…æ¥šã€‚

# Adapter AI Proxy (English Version)

This is an intelligent proxy server built with Python and Flask, designed for Large Language Model (LLM) backends like Ollama. Its core feature is the **Universal Adapter Architecture**, which allows it to be easily extended to support API formats from various clients (e.g., LobeChat, Cherry Studio).

This proxy can intelligently determine if a user's request includes an image, automatically coordinate a "thinking model" and a "vision model" to handle multimodal requests, and stream the formatted response back to the original client.

---

## Architecture

The core principle of this project is the **separation of "Core Logic" from "API Adaptation Logic."**

1.  **Core Logic**: This is the most valuable part of the project, responsible for handling all common tasks related to model interaction, such as:
    *   Determining if a request requires image analysis.
    *   Calling the vision model (`VISION_MODEL`) to describe an image.
    *   Combining the image description with the user's question and calling the thinking model (`THINKING_MODEL`) for a final answer.
    *   Handling streaming responses.

2.  **Adapters**: For each client application (AP) you want to support, there is a dedicated adapter. Each adapter is responsible for only two things:
    *   **Request Translation**: Translating the specific client's API request format (its "dialect") into the proxy's internal standard format.
    *   **Response Translation**: Converting the proxy's standard streaming response back into the format and API endpoint expected by the original client.

This design keeps the core logic stable and clean, while making it simple and low-risk to add support for new clients.

## Key Features

- **Intelligent Multimodal Handling**: Automatically detects images and uses a "describe-then-think" workflow for vision-language tasks.
- **Universal Adapter Architecture**: Quickly support new API formats by adding a new adapter in `adapters.py`.
- **Client Support**: Built-in support for:
  - **LobeChat** (`/api/chat`)
  - **OpenAI-compatible clients** (like Cherry Studio) (`/v1/chat/completions`)
- **Customizable Models**: Easily swap the thinking and vision models in `proxy_server.py`.
- **Streaming Responses**: Streams the final answer back to the client for a better user experience.

## Installation and Setup

1.  **Prerequisites**:
    *   Ensure you have [Ollama](https://ollama.com/) installed on your system.
    *   Pull the models required for this project. The default models are:
      ```bash
      ollama pull gpt-oss:20b # Thinking Model
      ollama pull gemma3:4b    # Vision Model
      ```
      *You can change the `THINKING_MODEL` and `VISION_MODEL` variables in `proxy_server.py`.*

2.  **Install Dependencies**:
    In the project's root directory, install the required Python packages using `requirements.txt`.
    ```bash
    pip install -r requirements.txt
    ```

## How to Use

1.  **Start the Proxy Server**:
    ```bash
    python proxy_server.py
    ```
    Once started, you will see the server listening on `http://localhost:5000`.

2.  **Configure Your Client**:
    Point your AI client (e.g., LobeChat) to this proxy server's address.

    - **For LobeChat**:
      - API Endpoint: `http://localhost:5000/api/chat`
      - Model Name: `gpt-oss:20b` (or your configured thinking model)

    - **For OpenAI-Compatible Clients**:
      - API Base URL: `http://localhost:5000/v1`
      - Model Name: `gpt-oss:20b`

## How to Support a New Client

Let's say you want to support a new client called "Future Assist." Just follow these steps:

1.  **Investigate its API Format**: Find out the API path (e.g., `/api/v3/chat`) and the JSON structure that "Future Assist" sends.

2.  **Create a New Adapter**: In the `adapters.py` file, create a new class that inherits from `BaseAdapter`.
    ```python
    class FutureAssistAdapter(BaseAdapter):
        name = "future_assist"

        def parse(self):
            # Implement the logic to parse the "Future Assist" JSON format here
            # ...
            return user_prompt, image_base64
    ```

3.  **Register the New Adapter**: In the `ADAPTERS` dictionary at the bottom of `adapters.py`, map the new client's API path keyword to the adapter class you just created.
    ```python
    ADAPTERS = {
        "api/chat": LobeChatAdapter,
        "v1/chat/completions": CherryStudioAdapter,
        "api/v3/chat": FutureAssistAdapter  # Add this line
    }
    ```

4.  **Restart the Server**: Relaunch `proxy_server.py` to apply the changes. It's that simple!

## ğŸ™ License & Acknowledgements

This project is open-source and uses various third-party libraries.
Please review their licenses carefully before using this code for commercial purposes.